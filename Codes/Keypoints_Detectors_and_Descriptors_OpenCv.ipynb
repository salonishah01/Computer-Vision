{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keypoints_Detectors_and_Descriptors_OpenCv.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPZYz_Khclyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "a96c9277-99eb-4d1a-94f0-9fe455639785"
      },
      "source": [
        "\"\"\"What are features, detectors, and keypoints?\n",
        "Features in an image are unique regions that the computer can easily tell apart.\n",
        "Corners are good examples of features. Finding these unique features is called feature detection.\n",
        "Once features are detected we need to find similar ones in a different image.\n",
        "This means we need to describe the features. \n",
        "Once you have the features and its description, you can find same features \n",
        "in all images and align them, stitch them or do whatever you want.\n",
        "Harris corner detector is a good example of feature detector. \n",
        "Keypoints are the same thing as points of interest. \n",
        "They are spatial locations, or points in the image that define what is interesting or what stand out in the image.\n",
        "The reason why keypoints are special is because no matter how the image changes...\n",
        "whether the image rotates, shrinks/expands, is translated, or distorted\n",
        "you should be able to find the same keypoints in this modified image when comparing with the original image.\n",
        "Harris corner detector detects corners. \n",
        "FAST: Features from Accelerated Segment Test - also detects corners\n",
        "Each keypoint that you detect has an associated descriptor that accompanies it.\n",
        "SIFT, SURF and ORB all detect and describe the keypoints.\n",
        "Descriptors are primarily concerned with both the scale and the orientation of the keypoint.\n",
        "e.g. run ORB and draw keypoints with rich keypoints.\n",
        "some of these points have a different circle radius. These deal with scale.\n",
        "The larger the \"circle\", the larger the scale was that the point was detected at.\n",
        "Also, there is a line that radiates from the centre of the circle to the edge. \n",
        "This is the orientation of the keypoint, which we will cover next\n",
        "Usually when we want to detect keypoints, we just take a look at the locations. \n",
        "However, if you want to match keypoints between images, \n",
        "then you definitely need the scale and the orientation to facilitate this.\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What are features, detectors, and keypoints?\\nFeatures in an image are unique regions that the computer can easily tell apart.\\nCorners are good examples of features. Finding these unique features is called feature detection.\\nOnce features are detected we need to find similar ones in a different image.\\nThis means we need to describe the features. \\nOnce you have the features and its description, you can find same features \\nin all images and align them, stitch them or do whatever you want.\\nHarris corner detector is a good example of feature detector. \\nKeypoints are the same thing as points of interest. \\nThey are spatial locations, or points in the image that define what is interesting or what stand out in the image.\\nThe reason why keypoints are special is because no matter how the image changes...\\nwhether the image rotates, shrinks/expands, is translated, or distorted\\nyou should be able to find the same keypoints in this modified image when comparing with the original image.\\nHarris corner detector detects corners. \\nFAST: Features from Accelerated Segment Test - also detects corners\\nEach keypoint that you detect has an associated descriptor that accompanies it.\\nSIFT, SURF and ORB all detect and describe the keypoints.\\nDescriptors are primarily concerned with both the scale and the orientation of the keypoint.\\ne.g. run ORB and draw keypoints with rich keypoints.\\nsome of these points have a different circle radius. These deal with scale.\\nThe larger the \"circle\", the larger the scale was that the point was detected at.\\nAlso, there is a line that radiates from the centre of the circle to the edge. \\nThis is the orientation of the keypoint, which we will cover next\\nUsually when we want to detect keypoints, we just take a look at the locations. \\nHowever, if you want to match keypoints between images, \\nthen you definitely need the scale and the orientation to facilitate this.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxfC0eWYc1AP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a06db95d-6b45-43b1-b3be-8f1bd7ad39c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjf1RNzbdWZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "img = cv2.imread('/content/drive/My Drive/CVG/grains.jpg')\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWZ3G0xceE55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gray = np.float32(gray)  #Harris works on float32 images. "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB7gS72xeIAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input parameters\n",
        "# image, block size (size of neighborhood considered), ksize (aperture parameter for Sobel), k\n",
        "harris = cv2.cornerHarris(gray,2,3,0.04)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvQ5FX-jePMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Threshold for an optimal value, it may vary depending on the image.\n",
        "img[harris>0.01*harris.max()]=[255,0,0]    # replace these pixels with blue\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E68wlMw_eTYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66367ff1-9e3b-456a-e507-b91368552885"
      },
      "source": [
        "\"\"\"\n",
        "cv2.imshow('Harris Corners',img)\n",
        "cv2.waitKey(0)\n",
        "\"\"\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncv2.imshow('Harris Corners',img)\\ncv2.waitKey(0)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZUQLwvPeY2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shi-Tomasi Corner Detector & Good Features to Track\n",
        "# In opencv it is called goodfeaturestotrack\n",
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qujedo8-fJZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = cv2.imread('/content/drive/My Drive/CVG/grains.jpg')\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cRr9_D7fPNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input image, #points, quality level (0-1), min euclidean dist. between detected points\n",
        "corners = cv2.goodFeaturesToTrack(gray,50,0.05,10)\n",
        "corners = np.int0(corners)   #np.int0 is int64"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n98_QCGCfWW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in corners:\n",
        "    x,y = i.ravel()   # Ravel Returns a contiguous flattened array.\n",
        "#    print(x,y)\n",
        "    cv2.circle(img,(x,y),3,255,-1)  #Draws circle (Img, center, radius, color, etc.)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TqgUKBzfbws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0f82678b-4cca-43a4-db08-4a2266cca68a"
      },
      "source": [
        "\"\"\"\n",
        "cv2.imshow('Corners',img)\n",
        "cv2.waitKey(0)\n",
        "\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncv2.imshow('Corners',img)\\ncv2.waitKey(0)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN_x66oIfkwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################\n",
        "#SIFT and SURF - do not work in opencv 3\n",
        "#SIFT stands for scale invariant feature transform\n",
        "#####################################\n",
        "# FAST\n",
        "# Features from Accelerated Segment Test\n",
        "# High speed corner detector\n",
        "# FAST is only keypoint detector. Cannot get any descriptors. \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGMoL6P-gO3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72381393-7840-4f05-d4b9-c50c963546af"
      },
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread('/content/drive/My Drive/CVG/grains.jpg', 0)\n",
        "\n",
        "# Initiate FAST object with default values\n",
        "detector = cv2.FastFeatureDetector_create(50)   #Detects 50 points\n",
        "\n",
        "kp = detector.detect(img, None)\n",
        "\n",
        "img2 = cv2.drawKeypoints(img, kp, None, flags=0)\n",
        "\"\"\"\n",
        "cv2.imshow('Corners',img2)\n",
        "cv2.waitKey(0)\n",
        "\"\"\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncv2.imshow('Corners',img2)\\ncv2.waitKey(0)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEq-GxTsgVpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############################################\n",
        "#BRIEF (Binary Robust Independent Elementary Features)\n",
        "#One important point is that BRIEF is a feature descriptor, \n",
        "#it doesn’t provide any method to find the features.\n",
        "###############################################"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBf1-LaVgu26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3784edad-35ad-449e-8a98-215a760ca039"
      },
      "source": [
        "#ORB\n",
        "# Oriented FAST and Rotated BRIEF\n",
        "# An efficient alternative to SIFT or SURF\n",
        "# ORB is basically a fusion of FAST keypoint detector and BRIEF descriptor\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread('/content/drive/My Drive/CVG/grains.jpg', 0)\n",
        "\n",
        "orb = cv2.ORB_create(100)\n",
        "\n",
        "\n",
        "kp, des = orb.detectAndCompute(img, None)\n",
        "\n",
        "\n",
        "# draw only keypoints location,not size and orientation\n",
        "#img2 = cv2.drawKeypoints(img, kp, None, flags=None)\n",
        "# Now, let us draw with rich key points, reflecting descriptors. \n",
        "# Descriptors here show both the scale and the orientation of the keypoint.\n",
        "img2 = cv2.drawKeypoints(img, kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\"\"\"\n",
        "cv2.imshow(\"With keypoints\", img2)\n",
        "cv2.waitKey(0)\n",
        "\"\"\"\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ncv2.imshow(\"With keypoints\", img2)\\ncv2.waitKey(0)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSw3q4IGg7ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}